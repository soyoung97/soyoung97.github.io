<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-25T20:44:13+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Soyoung’s blog</title><subtitle>Soyoung's blog that records about what I learned</subtitle><author><name>Soyoung Yoon</name></author><entry><title type="html">Pytorch Model 일부 Layer만 Freeze 하기</title><link href="http://localhost:4000/Pytorch-model-%EC%9D%BC%EB%B6%80-layer%EB%A7%8C-freeze-%ED%95%98%EA%B8%B0/" rel="alternate" type="text/html" title="Pytorch Model 일부 Layer만 Freeze 하기" /><published>2020-08-25T00:00:00+09:00</published><updated>2020-08-25T00:00:00+09:00</updated><id>http://localhost:4000/Pytorch-model-%EC%9D%BC%EB%B6%80-layer%EB%A7%8C-freeze-%ED%95%98%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/Pytorch-model-%EC%9D%BC%EB%B6%80-layer%EB%A7%8C-freeze-%ED%95%98%EA%B8%B0/">&lt;p&gt;task-specific한 Model training을 할 때, 기존의 pretrained model weight를
가져와서 하는 경우가 많이 있다.
여러 논문들에서도 BERT와 같이 pretrained된 대형 모델에서, layer 몇 개만
추가해주면 어떤 NLP task던 from scratch에서부터 training하는 것보다
좋은 성능을 낸다는 것을 입증하고 있다.&lt;/p&gt;

&lt;p&gt;지금 나는 classification model을 만들고 있는데, 역시 마찬가지로
huggingface에서 공개한 pretrained model을 사용 중이다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from transformers import *
bert = BertModel.from_pretrained('bert-base-uncased')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이 모델 뒤에, 간단한 linear - tanh - linear layer만 추가해서
model ouptut로 classification에 쓰는 label을 내도록 하기 위해선&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch.nn as nn
from transformers import *

class ClassificationModel(nn.Module):
    def __init__(self, pretrained_model='bert-base-uncased', num_labels=2):
        super(ClassificationModel, self).__init__()
        self.bert = BertModel.from_pretrained(pretrained_model)
        self.linear = nn.Sequential(nn.Linear(768, 128),
                                    nn.Tanh(),
                                    nn.Linear(128, num_labels))

    def forward(self, x):
        all_hidden, pooler = self.bert(**x)
        pooled_output = torch.mean(all_hidden, 1)
        predict = self.linear(pooled_output)
        return predict
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;요런 식으로 구현하면 된다.&lt;/p&gt;

&lt;p&gt;그런데!!! 문제가 생겼다.&lt;/p&gt;

&lt;p&gt;model training을 진행해 보니, 첫 번째 epoch를 돈 이후가 가장 성능이 좋았고,
그 뒤로부턴 성능이 계속계속 떨어졌다.&lt;/p&gt;

&lt;p&gt;문제가 뭐일까? 생각하는 와중에,
기존 pretrained model의 weight를 과도하게 bias시켜서(forgetting) 그런게 아닐까? 라는 생각이 들었다.&lt;/p&gt;

&lt;p&gt;그래서, 기존 pretrained model의 weight는 freeze시키고, 추가한 &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.Sequential&lt;/code&gt; layer의
weight만 training해보면 어떨까 라는생각이 들었다.&lt;/p&gt;

&lt;p&gt;방법은 의외로 간단했다.
일단 위의 코드를 선언한 다음&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; model = ClassificationModel()
&amp;gt;&amp;gt;&amp;gt; model.state_dict().keys()
odict_keys(['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight',
......'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'linear.0.weight', 'linear.0.bias', 'linear.2.weight', 'linear.2.bias'])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;를 해주면, 모델 안에 선언되어있는 다양한 weight들의 key값을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;여기서 우리가 바꿔주고 싶은 것은 오직 &lt;code class=&quot;highlighter-rouge&quot;&gt;'linear.0.weight', 'linear.0.bias', 'linear.2.weight', 'linear.2.bias'&lt;/code&gt;이것들!&lt;/p&gt;

&lt;p&gt;일단 모든 weight를 다 freeze시켜준다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;for para in model.parameters():
...     para.requires_grad = False
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그리고 해당 layer만 &lt;code class=&quot;highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt;를 켜주면 끝!
…..이라고 생각했지만 큰 오산이었다….왜냐..? 
&lt;code class=&quot;highlighter-rouge&quot;&gt;model.linear.0.weight&lt;/code&gt;에서 0이 .. .숫자가 있어서, 저런 방식으로 접근이 불가능했기 때문ㅠㅠㅠㅠㅠ&lt;/p&gt;

&lt;p&gt;만약에 이름이 &lt;code class=&quot;highlighter-rouge&quot;&gt;fc1&lt;/code&gt; 뭐 이런거였으면&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;model.fc1.weight.requires_grad = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;요렇게 해결해주면 되는 간단한 문제였지만,.. 불가능했기 때문에 다른 방법을 쓰기로 했다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;for name, param in model.named_parameters():
&amp;gt;&amp;gt;&amp;gt;    if name in ['linear.0.weight', 'linear.2.weight']:
&amp;gt;&amp;gt;&amp;gt;        param.requires_grad = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;요렇게 해서 해결하게 되었다…&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;추가: 원래 Model training할때 (꼭) 이렇게(만은) 하지는 않는다고 한다. 그리고, 내 모델은 forgetting이 되어서 성능이 내려갔다기 보다는 training data자체가 워낙 작아서,
이미 한epoch만에 수렴을 했고, 계속 overfitting되었던게 문제였던것같다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">task-specific한 Model training을 할 때, 기존의 pretrained model weight를 가져와서 하는 경우가 많이 있다. 여러 논문들에서도 BERT와 같이 pretrained된 대형 모델에서, layer 몇 개만 추가해주면 어떤 NLP task던 from scratch에서부터 training하는 것보다 좋은 성능을 낸다는 것을 입증하고 있다.</summary></entry><entry><title type="html">Mixup 기반 메소드들</title><link href="http://localhost:4000/Mixup-%EA%B8%B0%EB%B0%98-%EB%A9%94%EC%86%8C%EB%93%9C%EB%93%A4/" rel="alternate" type="text/html" title="Mixup 기반 메소드들" /><published>2020-08-11T00:00:00+09:00</published><updated>2020-08-11T00:00:00+09:00</updated><id>http://localhost:4000/Mixup-%EA%B8%B0%EB%B0%98-%EB%A9%94%EC%86%8C%EB%93%9C%EB%93%A4</id><content type="html" xml:base="http://localhost:4000/Mixup-%EA%B8%B0%EB%B0%98-%EB%A9%94%EC%86%8C%EB%93%9C%EB%93%A4/">&lt;p&gt;이번에 연구 주제와 관련되어서 &lt;a href=&quot;https://arxiv.org/pdf/1710.09412.pdf&quot;&gt;mixup&lt;/a&gt; 기반의 논문들을 읽어보고 공부하게 되었다.
여기서 중요하다고 느꼈던 점과 읽었던 부분들을 정리해보려고 한다.
시간이 지나면 다시 까먹을 것 같아서, 미리미리 정리해두는게 좋을것 같다.
(정리한 부분은 순전히 내가 읽고 받아들인 대로 적은 것이기 떄문에, 틀린 부분이 있을 수 있고,
잘못된 부분이 있다면 댓글로 알려주시면 정말 감사하겠습니다..ㅎㅎ)&lt;/p&gt;

&lt;p&gt;먼저 &lt;a href=&quot;https://arxiv.org/pdf/1710.09412.pdf&quot;&gt;mixup&lt;/a&gt;이라는 논문에서 말하고자 하는 요지는 간단하다.&lt;/p&gt;

&lt;p&gt;이 논문의 저자는 &lt;strong&gt;mixup&lt;/strong&gt;이라는 term을 사용하는데, 결국&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\tilde{x}%20=%20\lambda{x}_i%20+%20(1-\lambda){x}_j&quot; alt=&quot;equation&quot; /&gt;, where x_i, x_j are raw input vectors,
&lt;img src=&quot;https://latex.codecogs.com/gif.latex?\tilde{y}%20=%20\lambda{y}_i%20+%20(1-\lambda){y}_j&quot; alt=&quot;equation&quot; /&gt;, where y_i, y_j are one-hot label encodings&lt;/p&gt;

&lt;p&gt;이 수식이 논문의 모든 것을 설명해준다고 해도 과언이 아니다..(Markdown 으로 수식 표현하느라 힘들었다..)
즉, virtual example을 만드는데 있어서, x_i와 x_j를 일정한 비율(lambda)로 잘 섞어서, output label도 역시 섞어서 만들어준다는 것이다.&lt;/p&gt;

&lt;p&gt;이렇게 하면 뭐가 좋은가?&lt;/p&gt;

&lt;p&gt;이전의 training 기법들은 &lt;code class=&quot;highlighter-rouge&quot;&gt;ERM&lt;/code&gt;(Empirical Risk Minimization)을 중심으로 learning을 시켰다. ERM이란, 경험적으로 주어진 set X와 Y에 대해서만 risk를 최소화한다는 것으로, 이렇게 하게 된다면 Adversraial attack에 취약하게 되고, generalizaiton하는 대신
empirical data에 대한 memorize를 하게 되는 방향으로 training된다고 볼 수 있다.
따라서 이렇게 되는 것을 막기 위해 training data augmentation을 하게 되는데, 
이것을 &lt;code class=&quot;highlighter-rouge&quot;&gt;VRM&lt;/code&gt;(Vicinal Risk Minimization)이라고 한다.&lt;/p&gt;

&lt;p&gt;VRM이란, empirical data의 주변부(vicinity) 분포를 적절하게 모델링하고
이 data distribution에 대한 Risk를 minimization하는 것을 objective로 삼는 것이다. Semi-supervised learning의 한 종류로 봐도 좋을 것 같다.&lt;/p&gt;

&lt;p&gt;또 여기서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifold mixup&lt;/code&gt;라는 표현도 사용했는데, 처음에는 manifold가 무슨 뜻인지 몰라서 찾아보았다.
manifold를 간단히 설명하자면, 두 개의 data가 eucledian distance상으로는 가까이 있지만, 사실상 비유클리디안 형태의 분포로 있기 때문에 가까운게 멀수도있고, 먼게 가까울수도 있다는 말이다.
즉, manifold mixup이란 두 Input x1과 x2의 함수를 취한 결과값 f(x1) 과 f(x2) 두개를 위의 식과 같이 섞어준다는 간단한 말로 표현될 수 있다.&lt;/p&gt;

&lt;p&gt;여기까지가 ERM, VRM, 그리고 manifold mixup term에 대해 내가 이해한 바를 정리한 글이다.&lt;/p&gt;

&lt;p&gt;그럼 이게 [PuzzleMix]랑 어떻게 관련이 되는지를 소개하도록 하겠다.
Puzzlemix는 기존에 있었던 mixup을 기반으로 한 data augmentation을 수행하는데, 크게 두 가지의 contribution을 주장한다.
첫 번째는, 기존에는 무작위로 섞었다면, 여기서는 sailency를 각 이미지에서 뽑아서 그 sailency를 보존하는 이미지를 만든다.
두 번째로는, 각 이미지에 존재하는 local statistics를 보존한다.&lt;/p&gt;

&lt;p&gt;여기서 sailency란, “중요한 부분”을 나타내는 의미로서, 본문상에서는 vision분야에서는 foreground object, speech에서는 
prominant syallable, language에서는 informative textual unit등이 있다고 소개하고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/equations/puzzlemix.png&quot; alt=&quot;equation1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PuzzleMix에서 formulate하는 수식은 위와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/illustrations/puzzlemix1.png&quot; alt=&quot;equation1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 있는 절댓값에 씌여져 있는 두 term이 input data x0 와 x1에 해당하는 masking 및 optimal transport를 적용한 결과값이고,
나머지 beta, gamma, eta, alpha가 noise data에 대한 smoothness를 결정한다.
아래 그림의 첫 줄은 각 두 이미지를 얼마나 섞을 것인가(lambda)가 달라짐에 따라 달라지는 결과 output이고,&lt;/p&gt;

&lt;p&gt;두 번째 줄은 smoothness beta 와 mask r 를 얼마나 줄것인가 에 따라 달라지는 결과이다.&lt;/p&gt;

&lt;p&gt;즉, 만들어진 virtual image상에서 각 픽셀에 기존의 이미지를 얼마 비율로 섞을 것인가를 의미한다. 이것이 r의 값이다.
해당 픽셀에 r이 1/2이라면 output image는 두 Input image의 픽셀값이 골고루 둘다 들어가는 것이고, r이 0이거나 1이면
두 이미지 중 하나의 값만 들어가게 되는 것이다. 이렇게 골고루 적절히 잘 섞이게 하여 hyperparameter를 조정하여 이미지를
만들어낼 수 있다. 수식에서 pi에 해당하는 것은 각 이미지를 어떻게 optimal 하게 transport시킬 수 있는가에 대한 것이다.
(&lt;a href=&quot;https://mathematical-coffees.github.io/slides/mc01-courty.pdf&quot;&gt;optimal transport 관련 참고한 설명글&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;그 뒤에서 논문이 설명하는 것은 어떻게 하면 optimal transport를 빠르게 할 수 있는지(mini batch단위로 computation을 돌리는데 o(n^3)의
time complexity를 갖고 있기 때문에 그대로 적용하기 보다는 여러 가지 증명과정을 거쳐서 approximation이 가능하다는 것을 보이고
빠르게 계산하게 된다)를 증명하고 방법을 알려주고 있다.&lt;/p&gt;

&lt;p&gt;그 뒤는 이렇게 하는 것이 adversarial example을 만드는 것과 비슷한 효과를 낸다, 또는 adversarial example을 어떠한
추가적인 computational cost 없이 만드는 것이 가능하다는 이야기를 한다. (이 부분은 정확히 이해하지 못했다. 추후 더 읽고 이해한 후
수정할 예정이다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/illustrations/puzzlemix2.png&quot; alt=&quot;image1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문의 마지막은 Vanilla, Input, Manifold, CutMix, AugMix 등 다양한 mixup 기반의 메소드들 보다 PuzzleMix이 월등히 높은 성능 낸다는 것을 보여주고 있다.
puzzleMix(half)란, 기존의 training방법보다 총 iteration하는 epoch수를 반으로 줄이고 initial learning rate를 2배 해서 training을 돌린 것을 이야기한다.
computational cost 대비 fair comparison을 위해 추가하였다고 말하고 있다.
또, top-1 외에 top-2 dataset에 대해 다른 것들보다 훨씬 더 좋은 성능을 보인다는 것을 보이고 있다. 이것을 generalization을 잘한다고 말할 수 있을 것 같다.(나만의 결론이다. 틀릴 수도 있다..)&lt;/p&gt;

&lt;p&gt;여기서는 1.tiny-imagenet, 2.imagenet, 3.cifar-100 등 유명한 classification 위주의 dataset에 대해서 train해보았다고 말하고 있다.&lt;/p&gt;

&lt;p&gt;이걸 text에 바로 적용하기에는 문제가 있는데, 먼저 text는 image처럼 연속적인 픽셀값으로 이루어져 있는 것이 아니기 때문이다.
그렇다면 어떻게 해야 할까? mixup 기반의 method를 nlp에도 apply한 논문이 있는데, &lt;a href=&quot;https://arxiv.org/pdf/2004.12239.pdf&quot;&gt;mixtext&lt;/a&gt; 가 바로 그것이다.
이번 ACL 2020에 accept된 따끈따끈한 paper이다. 여기서 주요 contribution으로 주장하는 것은, mixup idea를 문장에 바로 적용할수가 없으니까,
hidden layer에 적용해보자! 이다. 이것은 어떻게 보면 manifold mixup의 한 형태로 볼 수 있을 것 같다.
이 아이디어는 논문 안에 있는 도식도를 보면 바로 이해가 잘 된다. 저자들은 이것을 mixtext로 칭하고 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/illustrations/mixtext.png&quot; alt=&quot;image2&quot; /&gt;&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">이번에 연구 주제와 관련되어서 mixup 기반의 논문들을 읽어보고 공부하게 되었다. 여기서 중요하다고 느꼈던 점과 읽었던 부분들을 정리해보려고 한다. 시간이 지나면 다시 까먹을 것 같아서, 미리미리 정리해두는게 좋을것 같다. (정리한 부분은 순전히 내가 읽고 받아들인 대로 적은 것이기 떄문에, 틀린 부분이 있을 수 있고, 잘못된 부분이 있다면 댓글로 알려주시면 정말 감사하겠습니다..ㅎㅎ)</summary></entry><entry><title type="html">논문 리뷰_pie</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_PIE/" rel="alternate" type="text/html" title="논문 리뷰_pie" /><published>2020-08-04T00:00:00+09:00</published><updated>2020-08-04T00:00:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_PIE</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_PIE/">&lt;h1 id=&quot;parallel-iterative-edit-models-for-local-sequence-transduction&quot;&gt;Parallel Iterative Edit Models for Local Sequence Transduction&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D19-1435.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-1.png&quot; alt=&quot;slide1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-2.png&quot; alt=&quot;slide2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-3.png&quot; alt=&quot;slide3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-4.png&quot; alt=&quot;slide4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-5.png&quot; alt=&quot;slide5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-6.png&quot; alt=&quot;slide6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-7.png&quot; alt=&quot;slide7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-8.png&quot; alt=&quot;slide8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-9.png&quot; alt=&quot;slide9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-10.png&quot; alt=&quot;slide10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-11.png&quot; alt=&quot;slide11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-12.png&quot; alt=&quot;slide12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-13.png&quot; alt=&quot;slide13&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-14.png&quot; alt=&quot;slide14&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-15.png&quot; alt=&quot;slide15&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/pie-16.png&quot; alt=&quot;slide16&quot; /&gt;&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">Parallel Iterative Edit Models for Local Sequence Transduction</summary></entry><entry><title type="html">논문 리뷰_unilm</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_unilm/" rel="alternate" type="text/html" title="논문 리뷰_unilm" /><published>2020-08-04T00:00:00+09:00</published><updated>2020-08-04T00:00:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_unilm</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_unilm/">&lt;h1 id=&quot;unified-language-model-pre-training-for-natural-language-understanding-and-generation&quot;&gt;Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1905.03197.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-1.png&quot; alt=&quot;slide1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-2.png&quot; alt=&quot;slide2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-3.png&quot; alt=&quot;slide3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-4.png&quot; alt=&quot;slide4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-5.png&quot; alt=&quot;slide5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-6.png&quot; alt=&quot;slide6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-7.png&quot; alt=&quot;slide7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-8.png&quot; alt=&quot;slide8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-9.png&quot; alt=&quot;slide9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/unilm-10.png&quot; alt=&quot;slide10&quot; /&gt;&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">Unified Language Model Pre-training for Natural Language Understanding and Generation pdf</summary></entry><entry><title type="html">Docker 컨테이너 Commit하고 Push하기</title><link href="http://localhost:4000/docker-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-commit%ED%95%98%EA%B3%A0-push%ED%95%98%EA%B8%B0/" rel="alternate" type="text/html" title="Docker 컨테이너 Commit하고 Push하기" /><published>2020-08-04T00:00:00+09:00</published><updated>2020-08-04T00:00:00+09:00</updated><id>http://localhost:4000/docker-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-commit%ED%95%98%EA%B3%A0-push%ED%95%98%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/docker-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-commit%ED%95%98%EA%B3%A0-push%ED%95%98%EA%B8%B0/">&lt;p&gt;&lt;a href=&quot;https://ai.nsml.navercorp.com/intro&quot;&gt;NSML&lt;/a&gt; 을 인턴 기간 중 사용하게 되면서 docker image를 띄우고, 컨테이너에 넣어서 수정하고, 다시 push해야하는 일이 생겼다.
나중에 까먹지 않도록, 각자 과정에 대해서 간단하게 기록해보려고 한다.&lt;/p&gt;

&lt;p&gt;기본적으로 docker run을 하게 되면 해당 container안에 있는 process가 다 돌아가게 되면 container는 exit하게 되고, 그 후에 다시 그 container를 run시킨다면 그 전에 작업했던 결과물들이 다 날라가게 된다(초기의 값으로 돌아가게 된다 TT)&lt;/p&gt;

&lt;p&gt;따라서 나는 &lt;code class=&quot;highlighter-rouge&quot;&gt;docker run -it [CONTAINER_NAME] [COMMAND(보통 /bin/bash)] &lt;/code&gt; 로 처음에 실행해놓은 docker를 &lt;code class=&quot;highlighter-rouge&quot;&gt;exit&lt;/code&gt; 시키지 않고 그 쉘은 작업이 끝날때까지 놔둔다…ㅎㅎ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;현재 돌고 있는 Container 확인: &lt;code class=&quot;highlighter-rouge&quot;&gt;docker ps -al&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@AL01333530 ~ % docker ps -al
CONTAINER ID        IMAGE                  COMMAND             CREATED             STATUS              PORTS               NAMES
6eb43f8766c0        msyoon8/fairseq-nsml   &quot;/bin/bash&quot;         8 hours ago         Up 8 hours                              sad_buck
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;해당 container “commit” 하기: &lt;code class=&quot;highlighter-rouge&quot;&gt;docker commit [CONTAINER_NAME] [NEW_IMAGE_NAME]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@AL01333530 ~ % docker commit sad_buck modified-nsml
sha256:59e86dfa8921492b020d0d258d05c47f0049e35141c291fe6e38cde645ee0fed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;만들어진 docker image 확인: &lt;code class=&quot;highlighter-rouge&quot;&gt;docker images&lt;/code&gt;를 하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@AL01333530 ~ % docker images
REPOSITORY             TAG                                      IMAGE ID            CREATED              SIZE
modified-nsml          latest                                   59e86dfa8921        About a minute ago   11GB
msyoon8/new-nsml       latest                                   106f53376a55        8 hours ago          11GB
start                  latest                                   7789300f795a        8 hours ago          10.9GB
msyoon8/fairseq-nsml   latest                                   7789300f795a        8 hours ago          10.9GB
&amp;lt;none&amp;gt;                 &amp;lt;none&amp;gt;                                   34ad76557739        9 hours ago          110MB
python                 alpine                                   bcf3965d8456        4 days ago           80.3MB
node                   12-alpine                                18f4bc975732        6 days ago           89.3MB
nginx                  alpine                                   ecd67fe340f9        3 weeks ago          21.6MB
msyoon8/nsml           start                                    d4cb5f857197        12 months ago        7.28GB
nsml/ml                cuda9.0-cudnn7-tf-1.11torch1.0keras2.2   d4cb5f857197        12 months ago        7.28GB
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;(예시입니다)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;docker login&lt;/code&gt; 하기 (나는 했으므로 pass)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;docker image에 tag 달기: &lt;code class=&quot;highlighter-rouge&quot;&gt;docker image tag [NEW_IMAGE_NAME]:latest [USER_NAME]/[NEW_IMAGE_NAME]:[WANTED_TAG]&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@AL01333530 ~ % docker image tag modified-nsml:latest msyoon8/modified-nsml:start
user@AL01333530 ~ % docker images
REPOSITORY              TAG                                      IMAGE ID            CREATED             SIZE
msyoon8/modified-nsml   start                                    59e86dfa8921        5 minutes ago       11GB
modified-nsml           latest                                   59e86dfa8921        5 minutes ago       11GB
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(참고: docker image tag를 지우고 싶다면 &lt;code class=&quot;highlighter-rouge&quot;&gt;docker rmi [IMAGE_NAME]:[TAG]&lt;/code&gt; -&amp;gt; actual image를 지우는건 아님)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;docker hub에 push: &lt;code class=&quot;highlighter-rouge&quot;&gt;docker push [USER_NAME]/[IMAGE_NAME]:[TAG]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@AL01333530 ~ % docker push msyoon8/modified-nsml:start
The push refers to repository [docker.io/msyoon8/modified-nsml]
29f8768524e6: Pushing [==================================================&amp;gt;]  24.02MB
345828a8dc7c: Mounted from msyoon8/new-nsml
4acf1ac205fc: Mounted from msyoon8/new-nsml
36bc5cc3b649: Mounted from msyoon8/new-nsml
8f4ebc5eb43a: Mounted from msyoon8/new-nsml
ebd4ce6c3d15: Waiting
59535ac5fd47: Waiting
2b023f1e40f7: Waiting
55a5c5838665: Waiting
68297ea5d26b: Waiting
6c0a7acf624e: Waiting
6c5aa0fe4fdd: Waiting
19021c46eb92: Waiting
01f46e800c43: Waiting
92d3f22d44f3: Waiting
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;시간이 조금 지나고 …..&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@AL01333530 ~ % docker push msyoon8/modified-nsml:start
The push refers to repository [docker.io/msyoon8/modified-nsml]
29f8768524e6: Pushed 
345828a8dc7c: Mounted from msyoon8/new-nsml 
4acf1ac205fc: Mounted from msyoon8/new-nsml 
36bc5cc3b649: Mounted from msyoon8/new-nsml 
8f4ebc5eb43a: Mounted from msyoon8/new-nsml 
ebd4ce6c3d15: Mounted from msyoon8/new-nsml 
59535ac5fd47: Mounted from msyoon8/new-nsml 
2b023f1e40f7: Mounted from msyoon8/new-nsml 
55a5c5838665: Mounted from msyoon8/new-nsml 
68297ea5d26b: Mounted from msyoon8/new-nsml 
6c0a7acf624e: Mounted from msyoon8/new-nsml 
6c5aa0fe4fdd: Mounted from msyoon8/new-nsml 
19021c46eb92: Mounted from msyoon8/new-nsml 
01f46e800c43: Mounted from msyoon8/new-nsml 
92d3f22d44f3: Mounted from msyoon8/new-nsml 
10e46f329a25: Mounted from msyoon8/new-nsml 
24ab7de5faec: Mounted from msyoon8/new-nsml 
1ea5a27b0484: Mounted from msyoon8/new-nsml 
start: digest: sha256:126df160f8899f7bf0bb4c21a724d41e39239afe27084229f78c06bb67277dcd size: 4116
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;요렇게 뜨면 제대로 push가 된 것이다!
&lt;img src=&quot;../assets/images/docker.jpg&quot; alt=&quot;dockerhub-image&quot; /&gt;
(docker hub에 제대로 push가 된 걸 볼 수 있다.)&lt;/p&gt;

&lt;p&gt;쨘!
그리고 이 이미지를 가져오면 된다 ㅎㅎ&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">NSML 을 인턴 기간 중 사용하게 되면서 docker image를 띄우고, 컨테이너에 넣어서 수정하고, 다시 push해야하는 일이 생겼다. 나중에 까먹지 않도록, 각자 과정에 대해서 간단하게 기록해보려고 한다.</summary></entry><entry><title type="html">논문 리뷰_electra</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_electra/" rel="alternate" type="text/html" title="논문 리뷰_electra" /><published>2020-08-03T00:00:00+09:00</published><updated>2020-08-03T00:00:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_electra</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_electra/">&lt;h1 id=&quot;electra-pre-training-text-encoders-as-discriminators-rather-than-generators&quot;&gt;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators&lt;/h1&gt;
&lt;h5 id=&quot;openreview-link&quot;&gt;&lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot;&gt;(openreview link)&lt;/a&gt;&lt;/h5&gt;

&lt;p&gt;첫 글은 연구실 논문리딩그룹에서 내가 발표했던 슬라이드를 활용하기로 하였다. 시간이 있을 때 설명을 더 추가할 예정!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/electra-1.png&quot; alt=&quot;slide1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/electra-2.png&quot; alt=&quot;slide2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/electra-3.png&quot; alt=&quot;slide3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/electra-4.png&quot; alt=&quot;slide4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/electra-5.png&quot; alt=&quot;slide5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/electra-6.png&quot; alt=&quot;slide6&quot; /&gt;&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (openreview link)</summary></entry><entry><title type="html">논문 리뷰_reformer</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_reformer/" rel="alternate" type="text/html" title="논문 리뷰_reformer" /><published>2020-08-03T00:00:00+09:00</published><updated>2020-08-03T00:00:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_reformer</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_reformer/">&lt;h1 id=&quot;reformer-the-efficient-transformer&quot;&gt;Reformer: the efficient Transformer&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2001.04451.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;처음에 이 논문이 나왔을떄는 정말 놀랐다. locality-sensitive hashing을 이용해서
원래 sequence length의 제곱에 비례하는 attention 과정의 time complexity를
linear하게 단축시킬 수 있다니! 후속 연구가 활발히 이루어져서 더 발전되었으면 좋겠다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-1.png&quot; alt=&quot;slide1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-2.png&quot; alt=&quot;slide2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-3.png&quot; alt=&quot;slide3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-4.png&quot; alt=&quot;slide4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-5.png&quot; alt=&quot;slide5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-6.png&quot; alt=&quot;slide6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-7.png&quot; alt=&quot;slide7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-8.png&quot; alt=&quot;slide8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-9.png&quot; alt=&quot;slide9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/reformer-10.png&quot; alt=&quot;slide10&quot; /&gt;&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">Reformer: the efficient Transformer pdf</summary></entry><entry><title type="html">논문 리뷰_grammatical Error Correction</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_Grammatical-Error-Correction/" rel="alternate" type="text/html" title="논문 리뷰_grammatical Error Correction" /><published>2020-08-03T00:00:00+09:00</published><updated>2020-08-03T00:00:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_Grammatical-Error-Correction</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0_Grammatical-Error-Correction/">&lt;h1 id=&quot;improving-grammatical-error-correction-via-pre-training-a-copy-augmented-architecture-with-unlabeled-data&quot;&gt;Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/N19-1014.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-1.png&quot; alt=&quot;slide1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-2.png&quot; alt=&quot;slide2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-3.png&quot; alt=&quot;slide3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-4.png&quot; alt=&quot;slide4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-5.png&quot; alt=&quot;slide5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-6.png&quot; alt=&quot;slide6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-7.png&quot; alt=&quot;slide7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-8.png&quot; alt=&quot;slide8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-9.png&quot; alt=&quot;slide9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-10.png&quot; alt=&quot;slide10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-11.png&quot; alt=&quot;slide11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-12.png&quot; alt=&quot;slide12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-13.png&quot; alt=&quot;slide13&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/slides/copyattn-14.png&quot; alt=&quot;slide14&quot; /&gt;&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data</summary></entry><entry><title type="html">Initial Post</title><link href="http://localhost:4000/initial-post/" rel="alternate" type="text/html" title="Initial Post" /><published>2020-08-02T00:00:00+09:00</published><updated>2020-08-02T00:00:00+09:00</updated><id>http://localhost:4000/initial-post</id><content type="html" xml:base="http://localhost:4000/initial-post/">&lt;p&gt;This is the first post!&lt;/p&gt;</content><author><name>Soyoung Yoon</name></author><summary type="html">This is the first post!</summary></entry></feed>