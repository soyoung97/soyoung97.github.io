var store = [{
        "title": "Initial Post",
        "excerpt":"This is the first post!  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/initial-post/",
        "teaser": null
      },{
        "title": "Grammatical Error Correction",
        "excerpt":"Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data   pdf                                            ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Grammatical-Error-Correction/",
        "teaser": null
      },{
        "title": "Electra",
        "excerpt":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generaors  (openreview link)   첫 글은 연구실 논문리딩그룹에서 내가 발표했던 슬라이드를 활용하기로 하였다. 시간이 있을 때 설명을 더 추가할 예정!                    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/electra/",
        "teaser": null
      },{
        "title": "Reformer",
        "excerpt":"Reformer: the efficient Transformer  pdf   처음에 이 논문이 나왔을떄는 정말 놀랐다. locality-sensitive hashing을 이용해서 원래 sequence length의 제곱에 비례하는 attention 과정의 time complexity를 linear하게 단축시킬 수 있다니! 후속 연구가 활발히 이루어져서 더 발전되었으면 좋겠다.                                 ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/reformer/",
        "teaser": null
      },{
        "title": "Pie",
        "excerpt":"Parallel Iterative Edit Models for Local Sequence Transduction   pdf                                                  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/pie/",
        "teaser": null
      },{
        "title": "Unilm",
        "excerpt":"Unified Language Model Pre-training for Natural Language Understanding and Generation  pdf                                ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/unilm/",
        "teaser": null
      }]
