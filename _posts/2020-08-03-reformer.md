# Reformer: the efficient Transformer
[pdf](https://arxiv.org/pdf/2001.04451.pdf)

처음에 이 논문이 나왔을떄는 정말 놀랐다. locality-sensitive hashing을 이용해서
원래 sequence length의 제곱에 비례하는 attention 과정의 time complexity를
linear하게 단축시킬 수 있다니! 후속 연구가 활발히 이루어져서 더 발전되었으면 좋겠다.


![slide1](../assets/images/slides/reformer-1.png)

![slide2](../assets/images/slides/reformer-2.png)

![slide3](../assets/images/slides/reformer-3.png)

![slide4](../assets/images/slides/reformer-4.png)

![slide5](../assets/images/slides/reformer-5.png)

![slide6](../assets/images/slides/reformer-6.png)

![slide7](../assets/images/slides/reformer-7.png)

![slide8](../assets/images/slides/reformer-8.png)

![slide9](../assets/images/slides/reformer-9.png)

![slide10](../assets/images/slides/reformer-10.png)

